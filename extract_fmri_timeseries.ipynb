{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This notebook loads the preprocessed fMRI HCP data (nifti files) and uses nilearn tools to define an \n",
    "## atlas to parcellate the brain and extract the timeseries from the functional data accounting for cofounds. \n",
    "## Then it concatenates the time series from all subjects and saves them as a single output:\n",
    "## all_subjects_fmri_time_series.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that the HCP resting state data is a relatively long run (1200 volumes) loading the whole dataset \n",
    "# occupies a big amount of RAM memory. If you are running this in your local machine this may cause it to crash.\n",
    "# One way to deal with this problem is to segment the data into smaller chunks of data and\n",
    "# load each chunk separatelly to extract the time series from it and then concatenate them\n",
    "# An easy way to do that is using the fslroi function from fsl, by embedding it into a for loop. \n",
    "# The following lines of bash code show how you could segment the data this way\n",
    "\n",
    "# First you will need to rename the nifti files for each subject\n",
    "\n",
    "# example for the first subject:   \n",
    "#mv rfMRI_REST1_LR.nii.gz 001.nii.gz\n",
    "\n",
    "# Then use fslroi in a for loop to segment the data of each subject into 6 chunks, 200 volumes each\n",
    "##!/bin/sh\n",
    "# for filename in *.nii.gz ; do\n",
    "#   fname=`$FSLDIR/bin/remove_ext ${filename}`\n",
    "#   fslroi ${filename} ${filename}_chunk_01 0 200\n",
    "#   fslroi ${filename} ${filename}_chunk_02 200 200\n",
    "#   fslroi ${filename} ${filename}_chunk_03 400 200\n",
    "#   fslroi ${filename} ${filename}_chunk_04 600 200\n",
    "#   fslroi ${filename} ${filename}_chunk_05 800 200\n",
    "#   fslroi ${filename} ${filename}_chunk_06 1000 200\n",
    "# done\n",
    "\n",
    "# Once you have prepared the data, you can proceed and run the next cells of this Jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up the dependencies we will be using for running this code\n",
    "import numpy as np\n",
    "from nilearn import datasets \n",
    "from nilearn import plotting\n",
    "from numpy import loadtxt\n",
    "from nilearn.input_data import NiftiLabelsMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we define the atlas to extract timeseries (here Destrieux altas 2009)\n",
    "dataset = datasets.fetch_atlas_destrieux_2009(lateralized=True) # Select specific altas from downloaded folder\n",
    "atlas_filename = dataset.maps # Define atlas image\n",
    "labels = dataset.labels # Set atlas labels that wull be used for plotting\n",
    "print('Atlas ROIs are located at: %s' % atlas_filename) # Print path to atlas\n",
    "\n",
    "# Plot atlas with title\n",
    "plotting.plot_roi(atlas_filename, title=\"Destrieux atlas\")\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create variables containing the elements for the paths to access the data\n",
    "gen_path = \"/media/jonathan/Data/McGill/HCP/\" # general path in my pc for the HCP data \n",
    "subject_id = [\"100307\", \"102816\", \"105923\", \"106521\", \"108323\", \"109123\", \"111514\", \"112920\", \"113922\", \"116524\", \"116726\", \"133019\"] # HCP subject id\n",
    "subject = [\"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\", \"010\", \"011\", \"012\"] # subject id\n",
    "chunks = [\"chunk_01\", \"chunk_02\", \"chunk_03\", \"chunk_04\", \"chunk_05\", \"chunk_06\"] # fMRI data (1200 vols) divided into 6 chunks of 200 vols each\n",
    "allsub_time_series = np.zeros((1200,148)) # Pre-allocate matrix\n",
    "\n",
    "# Start for loop to load the confounds txt file from each subject\n",
    "for i in range(12):\n",
    "    conf_path = gen_path+subject_id[i]+\"/Movement_Regressors.txt\"\n",
    "    conf = loadtxt(conf_path)\n",
    "    print(conf)\n",
    "    print(conf.shape)\n",
    "\n",
    "    # Separate the confound files into chunks that match the fMRI data\n",
    "    mov_reg1 = conf[range(0,200),:]\n",
    "    mov_reg2 = conf[range(200,400),:]\n",
    "    mov_reg3 = conf[range(400,600),:]\n",
    "    mov_reg4 = conf[range(600,800),:]\n",
    "    mov_reg5 = conf[range(800,1000),:]\n",
    "    mov_reg6 = conf[range(1000,1200),:]\n",
    "\n",
    "\n",
    "    # create paths to load the nifti files from each subject, chunk by chunk\n",
    "    filenames = []\n",
    "    for j in range(0,len(chunks)):\n",
    "        path = gen_path+subject_id[i]+\"/\"+subject[i]+\"_\"+chunks[j]+\".nii.gz\"\n",
    "        print(path)\n",
    "        filenames.append(path)\n",
    "    print(filenames)\n",
    "\n",
    "    # Extract time series accounting for cofounds, create concatenated matrix\n",
    "    masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True,\n",
    "                               memory='nilearn_cache', verbose=5)\n",
    "\n",
    "    # Extract the signal time series of each chunk \n",
    "    time_series1_c = masker.fit_transform(filenames[0], confounds=mov_reg1)\n",
    "    time_series2_c = masker.fit_transform(filenames[1], confounds=mov_reg2)\n",
    "    time_series3_c = masker.fit_transform(filenames[2], confounds=mov_reg3)\n",
    "    time_series4_c = masker.fit_transform(filenames[3], confounds=mov_reg4)\n",
    "    time_series5_c = masker.fit_transform(filenames[4], confounds=mov_reg5)\n",
    "    time_series6_c = masker.fit_transform(filenames[5], confounds=mov_reg6)\n",
    "\n",
    "    # Concatenate the signal timeseries  \n",
    "    time_series_c = np.concatenate((time_series1_c,time_series2_c,time_series3_c,time_series4_c,time_series5_c,time_series6_c))\n",
    "    print(type(time_series_c))\n",
    "    print(time_series_c.shape)\n",
    "\n",
    "    # Add this subject timeseries (148x1200 matrix) to a single array containing the timeseries from all subjects\n",
    "    allsub_time_series = np.concatenate((allsub_time_series,time_series_c))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
